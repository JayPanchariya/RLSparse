{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 16:56:27.042366: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#import driver\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "#import environment\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_py_environment #allows parallel computing for generating experiences\n",
    "from tf_agents.environments.parallel_py_environment import ParallelPyEnvironment\n",
    "\n",
    "#import replay buffer\n",
    "from tf_agents import replay_buffers as rb\n",
    "\n",
    "#import agent\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.utils import value_ops\n",
    "from tf_agents.trajectories import StepType\n",
    "\n",
    "\n",
    "#other used packages\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib.axes import Axes as ax\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf_agents\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.networks.categorical_projection_network import CategoricalProjectionNetwork\n",
    "import os,gc\n",
    "import pygad\n",
    "from pyswarms.single.global_best import GlobalBestPSO\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import environmentRL as ENV\n",
    "from typing import Tuple\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(tf.keras.Model):\n",
    "  \"\"\"Combined actor-critic network.\"\"\"\n",
    "\n",
    "  def __init__(self,num_actions: int, num_hidden_units: int):\n",
    "    \"\"\"Initialize.\"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n",
    "    self.actor = layers.Dense(num_actions)\n",
    "    self.critic = layers.Dense(1)\n",
    "\n",
    "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    x = self.common(inputs)\n",
    "    return self.actor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dddd\n",
      "initial state : [[-1.  1.]], reward: 0.0\n",
      "state_0 : [[-1.  1.]], action_0: [[-1.0518283  0.5030183]]\n",
      " normalized_act: [[-1.        -0.9999998]]\n",
      "reward:-6.5281219482421875\n",
      "state_1 : [[-1.05  0.95]], action_1: [[-1.0599902  0.5069216]]\n",
      " normalized_act: [[-1.         -0.99999976]]\n",
      "reward:-14.019988059997559\n",
      "state_2 : [[-1.0999999  0.9      ]], action_2: [[-1.0681522  0.5108249]]\n",
      " normalized_act: [[-1.         -0.99999976]]\n",
      "reward:-26.948097229003906\n",
      "state_3 : [[-1.1499999   0.84999996]], action_3: [[-1.0763141  0.5147282]]\n",
      " normalized_act: [[-1.        -0.9999998]]\n",
      "reward:-45.79994583129883\n",
      "state_4 : [[-1.1999998   0.79999995]], action_4: [[-1.084476   0.5186315]]\n",
      " normalized_act: [[-1.         -0.99999976]]\n",
      "reward:-71.07803344726562\n",
      "state_5 : [[-1.2499998   0.74999994]], action_5: [[-1.0926379  0.5225348]]\n",
      " normalized_act: [[-1.         -0.99999976]]\n",
      "reward:-103.29986572265625\n",
      "state_6 : [[-1.2999997  0.6999999]], action_6: [[-1.1007998   0.52643806]]\n",
      " normalized_act: [[-1.         -0.99999976]]\n",
      "reward:-142.9979248046875\n",
      "state_7 : [[-1.3499997  0.6499999]], action_7: [[-1.1089618  0.5303414]]\n",
      " normalized_act: [[-1.         -0.99999976]]\n",
      "reward:-190.7197265625\n",
      "state_8 : [[-1.3999996  0.5999999]], action_8: [[-1.1171237  0.5342447]]\n",
      " normalized_act: [[-1.         -0.99999976]]\n",
      "reward:-247.02777099609375\n",
      "state_9 : [[-1.4499996  0.5499999]], action_9: [[-1.1252856   0.53814805]]\n",
      " normalized_act: [[-1.         -0.99999976]]\n",
      "reward:-312.4995422363281\n"
     ]
    }
   ],
   "source": [
    "x0_reinforce = np.array([-1, 1],dtype=np.float32)\n",
    "env=ENV.Env(x0_reinforce)\n",
    "actor = Agent(2, 2)\n",
    "\n",
    "initS=env._reset()\n",
    "state = initS.observation[None, :]\n",
    "reward = initS.reward \n",
    "\n",
    "print(f\"initial state : {state}, reward: {reward}\")\n",
    "for i in range(10):\n",
    "    action = actor(state)\n",
    "    print(f\"state_{i} : {state}, action_{i}: {action}\")\n",
    "    s=env._step(action.numpy())\n",
    "    state = s.observation\n",
    "    reward = s.reward\n",
    "    print(f\"reward:{reward}\")\n",
    "    # print(s)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define lambda_coef\n",
    "lambda_coef = 0.5\n",
    "\n",
    "# Define the function\n",
    "def f(x):\n",
    "    return (x - 2)**2 + lambda_coef * (x**2)\n",
    "\n",
    "# Create x values\n",
    "x = np.linspace(-5, 5, 400)\n",
    "y = f(x)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x, y, label=r'$f(x) = (x-2)^2 + \\lambda x^2$', color='blue')\n",
    "plt.title('Objective Function Plot')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dm-reverb==0.9.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
