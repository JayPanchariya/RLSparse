{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 16:25:54.017960: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib.axes import Axes as ax\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf_agents\n",
    "import os,gc\n",
    "\n",
    "### Environment\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_py_environment #allows parallel computing for generating experiences\n",
    "from tf_agents.environments.parallel_py_environment import ParallelPyEnvironment\n",
    "\n",
    "\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.networks.categorical_projection_network import CategoricalProjectionNetwork\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.utils import common\n",
    "\n",
    "#import agent\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.utils import value_ops\n",
    "from tf_agents.trajectories import StepType\n",
    "\n",
    "#import replay buffer\n",
    "from tf_agents import replay_buffers as rb\n",
    "\n",
    "#import driver\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
    "\n",
    "# my function\n",
    "import environmentRL as envRL\n",
    "import function2D as fun\n",
    "import utills as utills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU') \n",
    "tf.config.experimental.set_visible_devices(cpus[0], 'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enable multiprocessing for parallel computing\n",
    "tf_agents.system.multiprocessing.enable_interactive_mode()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lr_schedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_lr, C):\n",
    "        self.initial_learning_rate = initial_lr\n",
    "        self.C = C\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        return self.initial_learning_rate * self.C / (self.C + step)\n",
    "     \n",
    "\n",
    "#Functions needed for training\n",
    "def extract_episode(traj_batch,epi_length,state_dim=2 ,attr_name = 'observation'):\n",
    "    \"\"\"\n",
    "    This function extract episodes (each episode consists of consecutive time_steps) from a batch of trajectories.\n",
    "    Inputs\n",
    "    -----------\n",
    "    traj_batch:replay_buffer.gather_all(), a batch of trajectories\n",
    "    epi_length:int, number of time_steps in each extracted episode\n",
    "    attr_name:str, specify which data from traj_batch to extract\n",
    "    \n",
    "    Outputs.\n",
    "    -----------\n",
    "    tf.constant(new_attr,dtype=attr.dtype), shape = [new_batch_size, epi_length, state_dim]\n",
    "                                        or shape = [new_batch_size, epi_length]\n",
    "    \"\"\"\n",
    "    attr = getattr(traj_batch,attr_name)\n",
    "    original_batch_dim = attr.shape[0]\n",
    "    traj_length = attr.shape[1]\n",
    "    epi_num = int(traj_length/epi_length) #number of episodes out of each trajectory\n",
    "    batch_dim = int(original_batch_dim*epi_num) #new batch_dim\n",
    "    \n",
    "    if len(attr.shape)==3:\n",
    "        stat_dim = attr.shape[2]\n",
    "        new_attr = np.zeros([batch_dim, epi_length, state_dim])\n",
    "    else:\n",
    "        new_attr = np.zeros([batch_dim, epi_length])\n",
    "        \n",
    "    for i in range(original_batch_dim):\n",
    "        for j in range(epi_num):\n",
    "            new_attr[i*epi_num+j] = attr[i,j*epi_length:(j+1)*epi_length].numpy()\n",
    "        \n",
    "    return tf.constant(new_attr,dtype=attr.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLOpt():\n",
    "    def __init__(self, start=0, end=3000, dirName='rosebourk'):\n",
    "        ### create a folder for  results and take an \n",
    "        self.path = os.path.join(\"results\", dirName)\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        \n",
    "        try:\n",
    "            os.makedirs(self.path, exist_ok=True)\n",
    "            os.makedirs(self.path+'/trajInt/', exist_ok=True)\n",
    "            print(\"Directory '%s' created successfully\" % dirName)\n",
    "        except OSError as error:\n",
    "            print(\"Directory '%s' can not be created\")\n",
    "            \n",
    "        N=2\n",
    "        self.state_dim =N\n",
    "        #Actor train program\n",
    "        self.REINFORCE_logs = [] #for logging the best objective value of the best solution among all the solutions used for one update of theta \n",
    "        self.eval_intv = 100 #number of updates required before each policy evaluation\n",
    "        self.final_reward = -1000\n",
    "        self.plot_intv = 100 # plot at interval \n",
    "        self.train_step_num = 0\n",
    "        #Set initial x-value\n",
    "        r = np.random.RandomState(0)\n",
    "        self.x0_reinforce = np.array([0.5,-0.5])\n",
    "        self.sub_episode_length = 50 #number of time_steps in a sub-episode. ## Batches for tejectories\n",
    "        \n",
    "        self.episode_length = self.sub_episode_length*6  #an trajectory starts from the initial timestep and has no other initial timesteps\n",
    "                                            #each trajectory will be split to multiple episodes\n",
    "        self.env_num = 40 #Number of parallel environments, each environment is used to generate an episode\n",
    "        print('x0', self.x0_reinforce)\n",
    "        \n",
    "        self.act_min = -3#-1\n",
    "        self.act_max = 3 #1\n",
    "        self.step_size = 0.05\n",
    "        self.step_numGe = 100 # gredient step size\n",
    "\n",
    "        #Set hyper-parameters for REINFORCE-OPT\n",
    "        self.generation_num = self.end #number of theta updates for REINFORCE-IP, also serves as the number\n",
    "                            #of generations for GA, and the number of iterations for particle swarm optimization\n",
    "        self.disc_factor = 1.0\n",
    "        self.alpha = 0.2 #regularization coefficient\n",
    "        self.param_alpha = 0.15 #regularization coefficient for actor_network #tested value: 0.02\n",
    "        self.sub_episode_num = int(self.env_num*(self.episode_length/self.sub_episode_length)) #number of sub-episodes used for a single update of actor_net params\n",
    "        print(\"number of sub_episodes used for a single param update:\", self.sub_episode_num)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.parallel_env = ParallelPyEnvironment(env_constructors=[lambda: envRL.Env(self.x0_reinforce,act_min=self.act_min,\n",
    "                                                                                      act_max=self.act_max, step_size=self.step_size,\n",
    "                                                                                      disc_factor=self.disc_factor,sub_episode_length=self.sub_episode_length,\n",
    "                                                                                      N=self.state_dim) for _ in range(self.env_num)],\n",
    "                                                  start_serially=False,\n",
    "                                                  blocking=False,\n",
    "                                                  flatten=False)\n",
    "        \n",
    "        #Use the wrapper to create a TFEnvironments obj. (so that parallel computation is enabled)\n",
    "        self.train_env = tf_py_environment.TFPyEnvironment(self.parallel_env, check_dims=True) #instance of parallel environments\n",
    "        self.eval_env = tf_py_environment.TFPyEnvironment(envRL.Env(self.x0_reinforce,act_min=self.act_min,\n",
    "                                                                                      act_max=self.act_max, step_size=self.step_size,\n",
    "                                                                                      disc_factor=self.disc_factor,sub_episode_length=self.sub_episode_length,\n",
    "                                                                                      N=self.state_dim), check_dims=False) #instance\n",
    "        # train_env.batch_size: The batch size expected for the actions and observations.  \n",
    "        print('train_env.batch_size = parallel environment number = ', self.env_num)\n",
    "\n",
    "        tf.random.set_seed(0)\n",
    "        self.actor_net = actor_distribution_network.ActorDistributionNetwork(   \n",
    "                                                self.train_env.observation_spec(),\n",
    "                                                self.train_env.action_spec(),\n",
    "                                                fc_layer_params=(16,16,16), #Hidden layers\n",
    "                                                seed=0, #seed used for Keras kernal initializers for NormalProjectionNetwork.\n",
    "                                                discrete_projection_net=CategoricalProjectionNetwork,\n",
    "                                                activation_fn = tf.math.tanh,\n",
    "                                                #continuous_projection_net=(NormalProjectionNetwork)\n",
    "                                                   )\n",
    "        \n",
    "        \n",
    "        lr = lr_schedule(initial_lr=0.00002, C=50000)   \n",
    "        self.opt = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "        \n",
    "        #Create the  REINFORCE_agent\n",
    "        train_step_counter = tf.Variable(0)\n",
    "        tf.random.set_seed(0)\n",
    "        self.REINFORCE_agent = reinforce_agent.ReinforceAgent(\n",
    "            time_step_spec = self.train_env.time_step_spec(),\n",
    "            action_spec = self.train_env.action_spec(),\n",
    "            actor_network = self.actor_net,\n",
    "            value_network = None,\n",
    "            value_estimation_loss_coef = 0.2,\n",
    "            optimizer = self.opt,\n",
    "            advantage_fn = None,\n",
    "            use_advantage_loss = False,\n",
    "            gamma = 1.0, #discount factor for future returns\n",
    "            normalize_returns = False, #The instruction says it's better to normalize\n",
    "            gradient_clipping = None,\n",
    "            entropy_regularization = None,\n",
    "            train_step_counter = train_step_counter\n",
    "            )\n",
    "\n",
    "        self.REINFORCE_agent.initialize()\n",
    "        \n",
    "         # Checkpoint setup\n",
    "        self.checkpoint_dir = 'checkpoints/'\n",
    "        self.checkpoint_prefix = os.path.join(self.checkpoint_dir, 'ckpt')\n",
    "        self.checkpoint = tf.train.Checkpoint(\n",
    "            step=tf.Variable(0),\n",
    "            optimizer=self.opt,\n",
    "            agent=self.REINFORCE_agent,\n",
    "            actor_network=self.actor_net\n",
    "        )\n",
    "        self.checkpoint_manager = tf.train.CheckpointManager(self.checkpoint, self.checkpoint_dir, max_to_keep=3)\n",
    "        if self.checkpoint_manager.latest_checkpoint:\n",
    "            self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint)\n",
    "            print(\"Restored from\", self.checkpoint_manager.latest_checkpoint)\n",
    "        else:\n",
    "            print(\"Initializing from scratch.\")\n",
    "        \n",
    "        #################\n",
    "        #replay_buffer is used to store policy exploration data\n",
    "        #################\n",
    "        self.replay_buffer = rb.TFUniformReplayBuffer(\n",
    "                data_spec = self.REINFORCE_agent.collect_data_spec,  # describe spec for a single iterm in the buffer. A TensorSpec or a list/tuple/nest of TensorSpecs describing a single item that can be stored in this buffer.\n",
    "                batch_size = self.env_num,    # number of parallel worlds, where in each world there is an agent generating trajectories\n",
    "                                         # One batch corresponds to one parallel environment\n",
    "                max_length = self.episode_length*100    # The maximum number of items that can be stored in a single batch segment of the buffer.     \n",
    "                                                    # if exceeding this number previous trajectories will be dropped\n",
    "                )\n",
    "        \n",
    "        #test_buffer is used for evaluating a policy\n",
    "        self.test_buffer = rb.TFUniformReplayBuffer(\n",
    "                data_spec= self.REINFORCE_agent.collect_data_spec,  # describe a single iterm in the buffer. A TensorSpec or a list/tuple/nest of TensorSpecs describing a single item that can be stored in this buffer.\n",
    "                batch_size= 1,    # number of parallel worlds, where in each world there is an agent generating trajectories\n",
    "                                                    # train_env.batch_size: The batch size expected for the actions and observations.  \n",
    "                                                    # batch_size: Batch dimension of tensors when adding to buffer. \n",
    "                max_length = self.episode_length         # The maximum number of items that can be stored in a single batch segment of the buffer.     \n",
    "                                                    # if exceeding this number previous trajectories will be dropped\n",
    "                )\n",
    "        \n",
    "            \n",
    "        #A driver uses an agent to perform its policy in the environment.\n",
    "        #The trajectory is saved in replay_buffer\n",
    "        self.collect_driver = DynamicEpisodeDriver(\n",
    "                                        env = self.train_env, #train_env contains parallel environments (no.: env_num)\n",
    "                                        policy = self.REINFORCE_agent.collect_policy,\n",
    "                                        observers = [self.replay_buffer.add_batch],\n",
    "                                        num_episodes = self.sub_episode_num   #SUM_i (number of episodes to be performed in the ith parallel environment)\n",
    "                                        )\n",
    "        \n",
    "        #For policy evaluation\n",
    "        self.test_driver = py_driver.PyDriver(\n",
    "                                     env = self.eval_env, #PyEnvironment or TFEnvironment class\n",
    "                                     policy = self.REINFORCE_agent.policy,\n",
    "                                     observers = [self.test_buffer.add_batch],\n",
    "                                     max_episodes=1, #optional. If provided, the data generation ends whenever\n",
    "                                                      #either max_steps or max_episodes is reached.\n",
    "                                     max_steps=self.sub_episode_length\n",
    "                                )\n",
    "\n",
    "            # self.parallel_env.close()\n",
    "            \n",
    "    def test_policy(self):\n",
    "        self.test_buffer.clear()\n",
    "        self.test_driver.run(self.eval_env.reset())  # run the current policy\n",
    "        experience = self.test_buffer.gather_all()\n",
    "        rl_trajectory = experience.observation.numpy()[0]\n",
    "        \n",
    "        ga_trajectory = self.gradientOpt()\n",
    "        utills.plotTrajectory(rl_trajectory, ga_trajectory, self.step_numGe, self.path)\n",
    "        \n",
    "    ############# - Create the Gradient Ascent Trajectory\n",
    "    def gradientOpt(self):\n",
    "        ga_trajectory = [self.x0_reinforce]\n",
    "        current_x = tf.Variable(self.x0_reinforce)\n",
    "        # current_x = tf.Variable([0.5,-1])\n",
    "        \n",
    "        for i in range(self.step_numGe):\n",
    "            with tf.GradientTape() as tape:\n",
    "                y = fun.f(current_x) ##-(2-tf.reduce_sum(tf.math.cos(10*current_x)) + 0.05*tf.reduce_sum(100*current_x**2))+10\n",
    "            gradient = tape.gradient(y, current_x)\n",
    "            norm_gradient = gradient.numpy()/np.sqrt(np.sum(gradient.numpy()**2))\n",
    "            current_x.assign(current_x.numpy() + norm_gradient*self.step_size)\n",
    "            ga_trajectory.append(current_x.numpy())\n",
    "        return ga_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'rosebourk' created successfully\n",
      "x0 [ 0.5 -0.5]\n",
      "number of sub_episodes used for a single param update: 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 16:27:10.912060: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:10.933372: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:11.162397: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:11.241225: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:11.261343: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:11.277272: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:12.025342: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:12.879806: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:13.365076: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:13.417672: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:13.669431: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:13.824459: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:13.952166: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:13.966715: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:14.052125: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:14.244187: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:14.426580: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:14.527978: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:14.616540: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:14.698063: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:14.906190: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:14.995821: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:15.077150: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:15.124788: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:15.297254: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:15.678631: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:15.831623: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:16.173043: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:16.265164: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:16.392511: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:16.473248: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:16.556273: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:16.972457: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:17.016650: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:17.579528: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:17.905266: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:18.074792: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:18.489934: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:18.696229: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-28 16:27:19.154623: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "RL=RLOpt()  \n",
    "\n",
    "update_num=RL.generation_num\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "RL.replay_buffer.clear()\n",
    "RL.collect_driver.run()  #a batch of trajectories will be saved in replay_buffer\n",
    "\n",
    "experience = RL.replay_buffer.gather_all() #get the batch of trajectories, shape=(batch_size, episode_length)\n",
    "rewards = extract_episode(traj_batch=experience,epi_length=RL.sub_episode_length,attr_name = 'reward') #shape=(sub_episode_num, sub_episode_length)\n",
    "observations = extract_episode(traj_batch=experience,epi_length=RL.sub_episode_length,attr_name = 'observation') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "actions = extract_episode(traj_batch=experience,epi_length=RL.sub_episode_length,attr_name = 'action') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "step_types = extract_episode(traj_batch=experience,epi_length=RL.sub_episode_length,attr_name = 'step_type')\n",
    "discounts = extract_episode(traj_batch=experience,epi_length=RL.sub_episode_length,attr_name = 'discount')\n",
    "\n",
    "time_steps = ts.TimeStep(step_types,\n",
    "                        tf.zeros_like(rewards),\n",
    "                        tf.zeros_like(discounts),\n",
    "                        observations) \n",
    "rewards_sum = tf.reduce_sum(rewards, axis=1) #shape=(sub_episode_num,\n",
    "\n",
    "\n",
    "actions_distribution = RL.REINFORCE_agent.collect_policy.distribution(\n",
    "                            time_steps, policy_state=None)\n",
    "\n",
    "\n",
    "print(actions_distribution)\n",
    "# ### for plotting contour import create 2D function \n",
    "# # X1, X2, Y=utills.create2Dfunction(x1Lim=(-2, 2), x2Lim=(-1, 3), N=200)\n",
    "# for n in np.arange(0, 1, 1):\n",
    "#     print(n)\n",
    "#     #Generate Trajectories\n",
    "#     RL.replay_buffer.clear()\n",
    "#     RL.collect_driver.run()  #a batch of trajectories will be saved in replay_buffer\n",
    "    \n",
    "#     experience = RL.replay_buffer.gather_all() #get the batch of trajectories, shape=(batch_size, episode_length)\n",
    "#     rewards = extract_episode(traj_batch=experience,epi_length=RL.sub_episode_length,attr_name = 'reward') #shape=(sub_episode_num, sub_episode_length)\n",
    "#     observations = extract_episode(traj_batch=experience,epi_length=RL.sub_episode_length,attr_name = 'observation') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "#     actions = extract_episode(traj_batch=experience,epi_length=RL.sub_episode_length,attr_name = 'action') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "#     step_types = extract_episode(traj_batch=experience,epi_length=RL.sub_episode_length,attr_name = 'step_type')\n",
    "#     discounts = extract_episode(traj_batch=experience,epi_length=RL.sub_episode_length,attr_name = 'discount')\n",
    "    \n",
    "#     time_steps = ts.TimeStep(step_types,\n",
    "#                         tf.zeros_like(rewards),\n",
    "#                         tf.zeros_like(discounts),\n",
    "#                         observations)\n",
    "\n",
    "#     rewards_sum = tf.reduce_sum(rewards, axis=1) #shape=(sub_episode_num,)\n",
    "\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         #trainable parameters in the actor_network in REINFORCE_agent\n",
    "#         variables_to_train = RL.REINFORCE_agent._actor_network.trainable_weights\n",
    "#         # tf.print(\"variables_to_train\", variables_to_train)\n",
    "#         ###########Compute J_loss = -J\n",
    "#         actions_distribution = RL.REINFORCE_agent.collect_policy.distribution(\n",
    "#                             time_steps, policy_state=None).action\n",
    "\n",
    "#         tf.print(\"actions_distribution\", actions_distribution)\n",
    "#         #log(pi(action|state)), shape = (batch_size, epsode_length)\n",
    "#         action_log_prob = common.log_probability(actions_distribution, \n",
    "#                                                 actions,\n",
    "#                                                 RL.REINFORCE_agent.action_spec)\n",
    "    \n",
    "#         J = tf.reduce_sum(tf.reduce_sum(action_log_prob,axis=1)*rewards_sum)/RL.sub_episode_num\n",
    "        \n",
    "#         ###########Compute regularization loss from actor_net params\n",
    "#         regu_term = tf.reduce_sum(variables_to_train[0]**2)\n",
    "#         num = len(variables_to_train) #number of vectors in variables_to_train\n",
    "#         for i in range(1,num):\n",
    "#             regu_term += tf.reduce_sum(variables_to_train[i]**2)\n",
    "        \n",
    "#         total = -J + RL.param_alactions_distributionpha*regu_term\n",
    "#     #update parameters in the actor_network in the policy\n",
    "#     grads = tape.gradient(total, variables_to_train)\n",
    "#     grads_and_vars = list(zip(grads, variables_to_train))\n",
    "#     RL.opt.apply_gradients(grads_and_vars=grads_and_vars)\n",
    "#     RL.train_step_num += 1\n",
    "    \n",
    "#     batch_rewards = rewards.numpy()\n",
    "#     batch_rewards[:,-1] = -np.power(10,8) #The initial reward is set as 0, we set it as this value to not affect the best_obs_index \n",
    "#     best_step_reward = np.max(batch_rewards)\n",
    "#     best_step_index = [int(batch_rewards.argmax()/RL.sub_episode_length),batch_rewards.argmax()%RL.sub_episode_length+1]\n",
    "#     best_step = observations[best_step_index[0],best_step_index[1],:] #best solution\n",
    "#     #best_step_reward = f(best_solution)\n",
    "#     avg_step_reward = np.mean(batch_rewards[:,0:-1])\n",
    "#     RL.REINFORCE_logs.append(best_step_reward)\n",
    "    \n",
    "#     if best_step_reward>RL.final_reward:\n",
    "#         #print(\"final reward before udpate:\",final_reward)\n",
    "#         RL.final_reward = best_step_reward\n",
    "#         final_solution = best_step.numpy()\n",
    "#         #print(\"final reward after udpate:\",final_reward)\n",
    "#         #print('updated final_solution=', final_solution)\n",
    "            \n",
    "#     if n%RL.eval_intv==0:\n",
    "#         print(\"train_step no.=\",RL.train_step_num)\n",
    "#         print('best_solution of this generation=', best_step.numpy())\n",
    "#         print('best step reward=',best_step_reward.round(3),fun.f(best_step.numpy()))\n",
    "#         print('avg step reward=', round(avg_step_reward,3))\n",
    "#         #print('episode of rewards', rewards.round(3))\n",
    "#         #print('act_std:', actions_distribution.stddev()[0,0]  )\n",
    "#         #print('act_mean:', actions_distribution.mean()[0,0] ) #second action mean\n",
    "#         print('best_step_index:',best_step_index)\n",
    "#         print(\"observation:\", observations[0])\n",
    "#         print(' ')\n",
    "#         # Save checkpoint\n",
    "#         RL.checkpoint.step.assign(n)\n",
    "#         RL.checkpoint_manager.save()\n",
    "#         print(\"Checkpoint saved at step\", int(RL.checkpoint.step))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob = actions_distribution\n",
    "\n",
    "print(log_prob.shape)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'actions_distribution' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mactions_distribution\u001b[49m\u001b[38;5;241m.\u001b[39maction\u001b[38;5;241m.\u001b[39mcdf)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mFigure()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'actions_distribution' is not defined"
     ]
    }
   ],
   "source": [
    "print(actions_distribution.action.cdf)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.Figure()\n",
    "plt.hist(actions_distribution[0].sample().numpy().ravel())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for plotting contour import create 2D function \n",
    "# X1, X2, Y=utills.create2Dfunction(x1Lim=(-2, 2), x2Lim=(-1, 3), N=200)\n",
    "for n in np.arange(0, 1, 1):\n",
    "    print(n)\n",
    "    #Generate Trajectories\n",
    "    RL.replay_buffer.clear()\n",
    "    RL.collect_driver.run()  #a batch of trajectories will be saved in replay_buffer\n",
    "    \n",
    "    experience = RL.replay_buffer.gather_all() #get the batch of trajectories, shape=(batch_size, episode_length)\n",
    "    rewards = extract_episode(traj_batch=experience,epi_length=RL.sub_episode_length,attr_name = 'reward') #shape=(sub_episode_num, sub_episode_length)\n",
    "    observations = extract_episode(traj_batch=experience,epi_length=RL.sub_episode_length,attr_name = 'observation') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "    actions = extract_episode(traj_batch=experience,epi_length=RL.sub_episode_length,attr_name = 'action') #shape=(sub_episode_num, sub_episode_length, state_dim)\n",
    "    step_types = extract_episode(traj_batch=experience,epi_length=RL.sub_episode_length,attr_name = 'step_type')\n",
    "    discounts = extract_episode(traj_batch=experience,epi_length=RL.sub_episode_length,attr_name = 'discount')\n",
    "    \n",
    "    time_steps = ts.TimeStep(step_types,\n",
    "                        tf.zeros_like(rewards),\n",
    "                        tf.zeros_like(discounts),\n",
    "                        observations)\n",
    "\n",
    "    rewards_sum = tf.reduce_sum(rewards, axis=1) #shape=(sub_episode_num,)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        #trainable parameters in the actor_network in REINFORCE_agent\n",
    "        variables_to_train = RL.REINFORCE_agent._actor_network.trainable_weights\n",
    "        # tf.print(\"variables_to_train\", variables_to_train)\n",
    "        ###########Compute J_loss = -J\n",
    "        actions_distribution = RL.REINFORCE_agent.collect_policy.distribution(\n",
    "                            time_steps, policy_state=None).action\n",
    "        \n",
    "        \n",
    "\n",
    "        tf.print(\"actions_distribution\", actions_distribution)\n",
    "        #log(pi(action|state)), shape = (batch_size, epsode_length)\n",
    "        action_log_prob = common.log_probability(actions_distribution, \n",
    "                                                actions,\n",
    "                                                RL.REINFORCE_agent.action_spec)\n",
    "    \n",
    "        J = tf.reduce_sum(tf.reduce_sum(action_log_prob,axis=1)*rewards_sum)/RL.sub_episode_num\n",
    "        \n",
    "        ###########Compute regularization loss from actor_net params\n",
    "        regu_term = tf.reduce_sum(variables_to_train[0]**2)\n",
    "        num = len(variables_to_train) #number of vectors in variables_to_train\n",
    "        for i in range(1,num):\n",
    "            regu_term += tf.reduce_sum(variables_to_train[i]**2)\n",
    "        \n",
    "        total = -J + RL.param_alactions_distributionpha*regu_term\n",
    "    #update parameters in the actor_network in the policy\n",
    "    grads = tape.gradient(total, variables_to_train)\n",
    "    grads_and_vars = list(zip(grads, variables_to_train))\n",
    "    RL.opt.apply_gradients(grads_and_vars=grads_and_vars)\n",
    "    RL.train_step_num += 1\n",
    "    \n",
    "    batch_rewards = rewards.numpy()\n",
    "    batch_rewards[:,-1] = -np.power(10,8) #The initial reward is set as 0, we set it as this value to not affect the best_obs_index \n",
    "    best_step_reward = np.max(batch_rewards)\n",
    "    best_step_index = [int(batch_rewards.argmax()/RL.sub_episode_length),batch_rewards.argmax()%RL.sub_episode_length+1]\n",
    "    best_step = observations[best_step_index[0],best_step_index[1],:] #best solution\n",
    "    #best_step_reward = f(best_solution)\n",
    "    avg_step_reward = np.mean(batch_rewards[:,0:-1])\n",
    "    RL.REINFORCE_logs.append(best_step_reward)\n",
    "    \n",
    "    if best_step_reward>RL.final_reward:\n",
    "        #print(\"final reward before udpate:\",final_reward)\n",
    "        RL.final_reward = best_step_reward\n",
    "        final_solution = best_step.numpy()\n",
    "        #print(\"final reward after udpate:\",final_reward)\n",
    "        #print('updated final_solution=', final_solution)\n",
    "            \n",
    "    if n%RL.eval_intv==0:\n",
    "        print(\"train_step no.=\",RL.train_step_num)\n",
    "        print('best_solution of this generation=', best_step.numpy())\n",
    "        print('best step reward=',best_step_reward.round(3),fun.f(best_step.numpy()))\n",
    "        print('avg step reward=', round(avg_step_reward,3))\n",
    "        #print('episode of rewards', rewards.round(3))\n",
    "        #print('act_std:', actions_distribution.stddev()[0,0]  )\n",
    "        #print('act_mean:', actions_distribution.mean()[0,0] ) #second action mean\n",
    "        print('best_step_index:',best_step_index)\n",
    "        print(\"observation:\", observations[0])\n",
    "        print(' ')\n",
    "        # Save checkpoint\n",
    "        RL.checkpoint.step.assign(n)\n",
    "        RL.checkpoint_manager.save()\n",
    "        print(\"Checkpoint saved at step\", int(RL.checkpoint.step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
